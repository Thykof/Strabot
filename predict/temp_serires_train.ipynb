{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the aim of this notebook is to prepare the dataset.\n",
    "We'd like to have multiple series of price.\n",
    "\n",
    "We only need one price per day (CLOSE for example)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5dn8YaIG44y_"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 7 0 0 0 0\n",
      "7 7 0 1 1 1\n",
      "7 7 0 2 2 2\n",
      "7 7 0 3 3 3\n",
      "7 7 0 4 4 4\n",
      "7 7 0 5 5 5\n",
      "7 7 0 6 6 6\n",
      "7 7 0 7 7 7\n",
      "7 7 0 8 8 8\n",
      "7 7 0 9 9 9\n",
      "7 7 0 10 10 10\n",
      "7 7 0 11 11 11\n",
      "7 7 0 12 12 12\n",
      "7 7 0 13 13 13\n",
      "7 7 0 14 14 14\n",
      "7 7 0 15 15 15\n",
      "7 7 0 16 16 16\n",
      "7 7 0 17 17 17\n",
      "7 7 0 18 18 18\n",
      "7 7 0 19 19 19\n",
      "7 7 0 20 20 20\n",
      "7 7 0 21 21 21\n",
      "7 7 0 22 22 22\n",
      "7 7 0 23 23 23\n",
      "7 7 0 24 24 24\n",
      "7 7 0 25 25 25\n",
      "7 7 0 26 26 26\n",
      "7 7 0 27 27 27\n",
      "7 7 0 28 28 28\n",
      "7 7 0 29 29 29\n",
      "7 7 0 30 30 30\n",
      "7 7 0 31 31 31\n",
      "7 7 0 32 32 32\n",
      "7 7 0 33 33 33\n",
      "7 7 0 34 34 34\n",
      "7 7 0 35 35 35\n",
      "7 7 0 36 36 36\n",
      "7 7 0 37 37 37\n",
      "7 7 0 38 38 38\n",
      "7 7 0 39 39 39\n",
      "7 7 0 40 40 40\n",
      "7 7 0 41 41 41\n",
      "7 7 0 42 42 42\n",
      "7 7 0 43 43 43\n",
      "7 7 0 44 44 44\n",
      "7 7 0 45 45 45\n",
      "7 7 0 46 46 46\n",
      "7 7 0 47 47 47\n",
      "7 7 0 48 48 48\n",
      "7 7 0 49 49 49\n",
      "7 7 0 50 50 50\n",
      "7 7 0 51 51 51\n",
      "7 7 0 52 52 52\n",
      "7 7 0 53 53 53\n",
      "7 6 1 0 1 1\n",
      "7 6 1 1 2 2\n",
      "7 6 1 2 3 3\n",
      "7 6 1 3 4 4\n",
      "7 6 1 4 5 5\n",
      "7 6 1 5 6 6\n",
      "7 6 1 6 7 7\n",
      "7 6 1 7 8 8\n",
      "7 6 1 8 9 9\n",
      "7 6 1 9 10 10\n",
      "7 6 1 10 11 11\n",
      "7 6 1 11 12 12\n",
      "7 6 1 12 13 13\n",
      "7 6 1 13 14 14\n",
      "7 6 1 14 15 15\n",
      "7 6 1 15 16 16\n",
      "7 6 1 16 17 17\n",
      "7 6 1 17 18 18\n",
      "7 6 1 18 19 19\n",
      "7 6 1 19 20 20\n",
      "7 6 1 20 21 21\n",
      "7 6 1 21 22 22\n",
      "7 6 1 22 23 23\n",
      "7 6 1 23 24 24\n",
      "7 6 1 24 25 25\n",
      "7 6 1 25 26 26\n",
      "7 6 1 26 27 27\n",
      "7 6 1 27 28 28\n",
      "7 6 1 28 29 29\n",
      "7 6 1 29 30 30\n",
      "7 6 1 30 31 31\n",
      "7 6 1 31 32 32\n",
      "7 6 1 32 33 33\n",
      "7 6 1 33 34 34\n",
      "7 6 1 34 35 35\n",
      "7 6 1 35 36 36\n",
      "7 6 1 36 37 37\n",
      "7 6 1 37 38 38\n",
      "7 6 1 38 39 39\n",
      "7 6 1 39 40 40\n",
      "7 6 1 40 41 41\n",
      "7 6 1 41 42 42\n",
      "7 6 1 42 43 43\n",
      "7 6 1 43 44 44\n",
      "7 6 1 44 45 45\n",
      "7 6 1 45 46 46\n",
      "7 6 1 46 47 47\n",
      "7 6 1 47 48 48\n",
      "7 6 1 48 49 49\n",
      "7 6 1 49 50 50\n",
      "7 6 1 50 51 51\n",
      "7 6 1 51 52 52\n",
      "7 6 1 52 53 53\n",
      "7 6 1 53 54 54\n",
      "7 5 2 0 2 2\n",
      "7 5 2 1 3 3\n",
      "7 5 2 2 4 4\n",
      "7 5 2 3 5 5\n",
      "7 5 2 4 6 6\n",
      "7 5 2 5 7 7\n",
      "7 5 2 6 8 8\n",
      "7 5 2 7 9 9\n",
      "7 5 2 8 10 10\n",
      "7 5 2 9 11 11\n",
      "7 5 2 10 12 12\n",
      "7 5 2 11 13 13\n",
      "7 5 2 12 14 14\n",
      "7 5 2 13 15 15\n",
      "7 5 2 14 16 16\n",
      "7 5 2 15 17 17\n",
      "7 5 2 16 18 18\n",
      "7 5 2 17 19 19\n",
      "7 5 2 18 20 20\n",
      "7 5 2 19 21 21\n",
      "7 5 2 20 22 22\n",
      "7 5 2 21 23 23\n",
      "7 5 2 22 24 24\n",
      "7 5 2 23 25 25\n",
      "7 5 2 24 26 26\n",
      "7 5 2 25 27 27\n",
      "7 5 2 26 28 28\n",
      "7 5 2 27 29 29\n",
      "7 5 2 28 30 30\n",
      "7 5 2 29 31 31\n",
      "7 5 2 30 32 32\n",
      "7 5 2 31 33 33\n",
      "7 5 2 32 34 34\n",
      "7 5 2 33 35 35\n",
      "7 5 2 34 36 36\n",
      "7 5 2 35 37 37\n",
      "7 5 2 36 38 38\n",
      "7 5 2 37 39 39\n",
      "7 5 2 38 40 40\n",
      "7 5 2 39 41 41\n",
      "7 5 2 40 42 42\n",
      "7 5 2 41 43 43\n",
      "7 5 2 42 44 44\n",
      "7 5 2 43 45 45\n",
      "7 5 2 44 46 46\n",
      "7 5 2 45 47 47\n",
      "7 5 2 46 48 48\n",
      "7 5 2 47 49 49\n",
      "7 5 2 48 50 50\n",
      "7 5 2 49 51 51\n",
      "7 5 2 50 52 52\n",
      "7 5 2 51 53 53\n",
      "7 5 2 52 54 54\n",
      "7 5 2 53 55 55\n",
      "7 4 3 0 3 3\n",
      "7 4 3 1 4 4\n",
      "7 4 3 2 5 5\n",
      "7 4 3 3 6 6\n",
      "7 4 3 4 7 7\n",
      "7 4 3 5 8 8\n",
      "7 4 3 6 9 9\n",
      "7 4 3 7 10 10\n",
      "7 4 3 8 11 11\n",
      "7 4 3 9 12 12\n",
      "7 4 3 10 13 13\n",
      "7 4 3 11 14 14\n",
      "7 4 3 12 15 15\n",
      "7 4 3 13 16 16\n",
      "7 4 3 14 17 17\n",
      "7 4 3 15 18 18\n",
      "7 4 3 16 19 19\n",
      "7 4 3 17 20 20\n",
      "7 4 3 18 21 21\n",
      "7 4 3 19 22 22\n",
      "7 4 3 20 23 23\n",
      "7 4 3 21 24 24\n",
      "7 4 3 22 25 25\n",
      "7 4 3 23 26 26\n",
      "7 4 3 24 27 27\n",
      "7 4 3 25 28 28\n",
      "7 4 3 26 29 29\n",
      "7 4 3 27 30 30\n",
      "7 4 3 28 31 31\n",
      "7 4 3 29 32 32\n",
      "7 4 3 30 33 33\n",
      "7 4 3 31 34 34\n",
      "7 4 3 32 35 35\n",
      "7 4 3 33 36 36\n",
      "7 4 3 34 37 37\n",
      "7 4 3 35 38 38\n",
      "7 4 3 36 39 39\n",
      "7 4 3 37 40 40\n",
      "7 4 3 38 41 41\n",
      "7 4 3 39 42 42\n",
      "7 4 3 40 43 43\n",
      "7 4 3 41 44 44\n",
      "7 4 3 42 45 45\n",
      "7 4 3 43 46 46\n",
      "7 4 3 44 47 47\n",
      "7 4 3 45 48 48\n",
      "7 4 3 46 49 49\n",
      "7 4 3 47 50 50\n",
      "7 4 3 48 51 51\n",
      "7 4 3 49 52 52\n",
      "7 4 3 50 53 53\n",
      "7 4 3 51 54 54\n",
      "7 4 3 52 55 55\n",
      "7 4 3 53 56 56\n",
      "7 3 4 0 4 4\n",
      "7 3 4 1 5 5\n",
      "7 3 4 2 6 6\n",
      "7 3 4 3 7 7\n",
      "7 3 4 4 8 8\n",
      "7 3 4 5 9 9\n",
      "7 3 4 6 10 10\n",
      "7 3 4 7 11 11\n",
      "7 3 4 8 12 12\n",
      "7 3 4 9 13 13\n",
      "7 3 4 10 14 14\n",
      "7 3 4 11 15 15\n",
      "7 3 4 12 16 16\n",
      "7 3 4 13 17 17\n",
      "7 3 4 14 18 18\n",
      "7 3 4 15 19 19\n",
      "7 3 4 16 20 20\n",
      "7 3 4 17 21 21\n",
      "7 3 4 18 22 22\n",
      "7 3 4 19 23 23\n",
      "7 3 4 20 24 24\n",
      "7 3 4 21 25 25\n",
      "7 3 4 22 26 26\n",
      "7 3 4 23 27 27\n",
      "7 3 4 24 28 28\n",
      "7 3 4 25 29 29\n",
      "7 3 4 26 30 30\n",
      "7 3 4 27 31 31\n",
      "7 3 4 28 32 32\n",
      "7 3 4 29 33 33\n",
      "7 3 4 30 34 34\n",
      "7 3 4 31 35 35\n",
      "7 3 4 32 36 36\n",
      "7 3 4 33 37 37\n",
      "7 3 4 34 38 38\n",
      "7 3 4 35 39 39\n",
      "7 3 4 36 40 40\n",
      "7 3 4 37 41 41\n",
      "7 3 4 38 42 42\n",
      "7 3 4 39 43 43\n",
      "7 3 4 40 44 44\n",
      "7 3 4 41 45 45\n",
      "7 3 4 42 46 46\n",
      "7 3 4 43 47 47\n",
      "7 3 4 44 48 48\n",
      "7 3 4 45 49 49\n",
      "7 3 4 46 50 50\n",
      "7 3 4 47 51 51\n",
      "7 3 4 48 52 52\n",
      "7 3 4 49 53 53\n",
      "7 3 4 50 54 54\n",
      "7 3 4 51 55 55\n",
      "7 3 4 52 56 56\n",
      "7 3 4 53 57 57\n",
      "7 2 5 0 5 5\n",
      "7 2 5 1 6 6\n",
      "7 2 5 2 7 7\n",
      "7 2 5 3 8 8\n",
      "7 2 5 4 9 9\n",
      "7 2 5 5 10 10\n",
      "7 2 5 6 11 11\n",
      "7 2 5 7 12 12\n",
      "7 2 5 8 13 13\n",
      "7 2 5 9 14 14\n",
      "7 2 5 10 15 15\n",
      "7 2 5 11 16 16\n",
      "7 2 5 12 17 17\n",
      "7 2 5 13 18 18\n",
      "7 2 5 14 19 19\n",
      "7 2 5 15 20 20\n",
      "7 2 5 16 21 21\n",
      "7 2 5 17 22 22\n",
      "7 2 5 18 23 23\n",
      "7 2 5 19 24 24\n",
      "7 2 5 20 25 25\n",
      "7 2 5 21 26 26\n",
      "7 2 5 22 27 27\n",
      "7 2 5 23 28 28\n",
      "7 2 5 24 29 29\n",
      "7 2 5 25 30 30\n",
      "7 2 5 26 31 31\n",
      "7 2 5 27 32 32\n",
      "7 2 5 28 33 33\n",
      "7 2 5 29 34 34\n",
      "7 2 5 30 35 35\n",
      "7 2 5 31 36 36\n",
      "7 2 5 32 37 37\n",
      "7 2 5 33 38 38\n",
      "7 2 5 34 39 39\n",
      "7 2 5 35 40 40\n",
      "7 2 5 36 41 41\n",
      "7 2 5 37 42 42\n",
      "7 2 5 38 43 43\n",
      "7 2 5 39 44 44\n",
      "7 2 5 40 45 45\n",
      "7 2 5 41 46 46\n",
      "7 2 5 42 47 47\n",
      "7 2 5 43 48 48\n",
      "7 2 5 44 49 49\n",
      "7 2 5 45 50 50\n",
      "7 2 5 46 51 51\n",
      "7 2 5 47 52 52\n",
      "7 2 5 48 53 53\n",
      "7 2 5 49 54 54\n",
      "7 2 5 50 55 55\n",
      "7 2 5 51 56 56\n",
      "7 2 5 52 57 57\n",
      "7 2 5 53 58 58\n",
      "7 1 6 0 6 6\n",
      "7 1 6 1 7 7\n",
      "7 1 6 2 8 8\n",
      "7 1 6 3 9 9\n",
      "7 1 6 4 10 10\n",
      "7 1 6 5 11 11\n",
      "7 1 6 6 12 12\n",
      "7 1 6 7 13 13\n",
      "7 1 6 8 14 14\n",
      "7 1 6 9 15 15\n",
      "7 1 6 10 16 16\n",
      "7 1 6 11 17 17\n",
      "7 1 6 12 18 18\n",
      "7 1 6 13 19 19\n",
      "7 1 6 14 20 20\n",
      "7 1 6 15 21 21\n",
      "7 1 6 16 22 22\n",
      "7 1 6 17 23 23\n",
      "7 1 6 18 24 24\n",
      "7 1 6 19 25 25\n",
      "7 1 6 20 26 26\n",
      "7 1 6 21 27 27\n",
      "7 1 6 22 28 28\n",
      "7 1 6 23 29 29\n",
      "7 1 6 24 30 30\n",
      "7 1 6 25 31 31\n",
      "7 1 6 26 32 32\n",
      "7 1 6 27 33 33\n",
      "7 1 6 28 34 34\n",
      "7 1 6 29 35 35\n",
      "7 1 6 30 36 36\n",
      "7 1 6 31 37 37\n",
      "7 1 6 32 38 38\n",
      "7 1 6 33 39 39\n",
      "7 1 6 34 40 40\n",
      "7 1 6 35 41 41\n",
      "7 1 6 36 42 42\n",
      "7 1 6 37 43 43\n",
      "7 1 6 38 44 44\n",
      "7 1 6 39 45 45\n",
      "7 1 6 40 46 46\n",
      "7 1 6 41 47 47\n",
      "7 1 6 42 48 48\n",
      "7 1 6 43 49 49\n",
      "7 1 6 44 50 50\n",
      "7 1 6 45 51 51\n",
      "7 1 6 46 52 52\n",
      "7 1 6 47 53 53\n",
      "7 1 6 48 54 54\n",
      "7 1 6 49 55 55\n",
      "7 1 6 50 56 56\n",
      "7 1 6 51 57 57\n",
      "7 1 6 52 58 58\n",
      "7 1 6 53 59 59\n",
      "     0   1   2   3   4   5   6\n",
      "0    0   1   2   3   4   5   6\n",
      "1    1   2   3   4   5   6   7\n",
      "2    2   3   4   5   6   7   8\n",
      "3    3   4   5   6   7   8   9\n",
      "4    4   5   6   7   8   9  10\n",
      "5    5   6   7   8   9  10  11\n",
      "6    6   7   8   9  10  11  12\n",
      "7    7   8   9  10  11  12  13\n",
      "8    8   9  10  11  12  13  14\n",
      "9    9  10  11  12  13  14  15\n",
      "10  10  11  12  13  14  15  16\n",
      "11  11  12  13  14  15  16  17\n",
      "12  12  13  14  15  16  17  18\n",
      "13  13  14  15  16  17  18  19\n",
      "14  14  15  16  17  18  19  20\n",
      "15  15  16  17  18  19  20  21\n",
      "16  16  17  18  19  20  21  22\n",
      "17  17  18  19  20  21  22  23\n",
      "18  18  19  20  21  22  23  24\n",
      "19  19  20  21  22  23  24  25\n",
      "20  20  21  22  23  24  25  26\n",
      "21  21  22  23  24  25  26  27\n",
      "22  22  23  24  25  26  27  28\n",
      "23  23  24  25  26  27  28  29\n",
      "24  24  25  26  27  28  29  30\n",
      "25  25  26  27  28  29  30  31\n",
      "26  26  27  28  29  30  31  32\n",
      "27  27  28  29  30  31  32  33\n",
      "28  28  29  30  31  32  33  34\n",
      "29  29  30  31  32  33  34  35\n",
      "30  30  31  32  33  34  35  36\n",
      "31  31  32  33  34  35  36  37\n",
      "32  32  33  34  35  36  37  38\n",
      "33  33  34  35  36  37  38  39\n",
      "34  34  35  36  37  38  39  40\n",
      "35  35  36  37  38  39  40  41\n",
      "36  36  37  38  39  40  41  42\n",
      "37  37  38  39  40  41  42  43\n",
      "38  38  39  40  41  42  43  44\n",
      "39  39  40  41  42  43  44  45\n",
      "40  40  41  42  43  44  45  46\n",
      "41  41  42  43  44  45  46  47\n",
      "42  42  43  44  45  46  47  48\n",
      "43  43  44  45  46  47  48  49\n",
      "44  44  45  46  47  48  49  50\n",
      "45  45  46  47  48  49  50  51\n",
      "46  46  47  48  49  50  51  52\n",
      "47  47  48  49  50  51  52  53\n",
      "48  48  49  50  51  52  53  54\n",
      "49  49  50  51  52  53  54  55\n",
      "50  50  51  52  53  54  55  56\n",
      "51  51  52  53  54  55  56  57\n",
      "52  52  53  54  55  56  57  58\n",
      "53  53  54  55  56  57  58  59\n"
     ]
    }
   ],
   "source": [
    "def prepare_tab(tab, n):\n",
    "\n",
    "    df = pd.DataFrame(eval( '[' + 'np.arange(n),' * (len(tab)-n+1) + ']' ))\n",
    "    \n",
    "    p = n\n",
    "    while p > 0:\n",
    "        i = n-p\n",
    "        for k in range(len(tab)-n+1):\n",
    "            print(n, p, n-p, k, i, tab[i])\n",
    "            df[n-p]\\\n",
    "            [k] = \\\n",
    "            tab[i]\n",
    "            i += 1\n",
    "        p -= 1\n",
    "    \n",
    "    return df\n",
    "\n",
    "print(prepare_tab(list(range(60)), 7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'en_US.UTF-8'"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import tree, svm, linear_model, neural_network, gaussian_process\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, TimeSeriesSplit, GridSearchCV, ParameterGrid, cross_validate\n",
    "from sklearn import preprocessing\n",
    "from sklearn import metrics\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import locale\n",
    "locale.setlocale(locale.LC_ALL, 'en_US.UTF-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "TIME = 'time'\n",
    "OPEN = 'open'\n",
    "HIGH = 'high'\n",
    "LOW = 'low'\n",
    "CLOSE = 'close'\n",
    "VWAP = 'vwap'\n",
    "VOLUME = 'volume'\n",
    "COUNT = 'count'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the dataset\n",
    "df_num = pd.read_csv(\"ohlc-btc-eur-day.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OR open this dataset\n",
    "CLOSE = \"Close**\"\n",
    "df = pd.read_csv(\"bitcoin-price-all-2.csv\", sep=',')\n",
    "# Convert to numeric\n",
    "def raw_data_to_numeric(raw_data):\n",
    "    data = raw_data.replace('â€¯', '')\n",
    "    data = data.replace(',', '.')\n",
    "    if data == '0':\n",
    "        return np.nan\n",
    "    return float(data)\n",
    "df_num = df[['Open*', 'High', 'Low', 'Close**', 'Volume', 'Market Cap']]\n",
    "df_num = df_num.applymap(raw_data_to_numeric)\n",
    "# Replace NaN by first known data (for `Volume`)\n",
    "df_num = df_num.fillna(46862700)\n",
    "# Read dates\n",
    "dates = df['Date'].apply(lambda x: datetime.datetime.strptime(x, '%b %d, %Y'))\n",
    "dates_num = dates.map(datetime.datetime.toordinal)\n",
    "df_dated = df_num.copy()\n",
    "df_dated.insert(0, 'Date', dates)\n",
    "df_dated.sort_values(by=['Date'], ascending=True)\n",
    "df_num = df_dated.copy()[['Open*', 'High', 'Low', 'Close**', 'Volume', 'Market Cap']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select columns\n",
    "df_num = df_num[[CLOSE]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter\n",
    "n = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Prepare function\n",
    "\n",
    "def prepare(tab, n):\n",
    "    df_ = pd.DataFrame(eval( '[' + 'np.arange(n)*0.0,' * (len(tab)-n+1) + ']' ))\n",
    "    \n",
    "    p = n\n",
    "    while p > 0:\n",
    "        i = n-p\n",
    "        for k in range(len(tab)-n+1):\n",
    "            df_[n-p][k] = float(tab[i])\n",
    "            i += 1\n",
    "        p -= 1\n",
    "    \n",
    "    return df_\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      8527.8\n",
      "1      8364.9\n",
      "2      8062.1\n",
      "3      8163.3\n",
      "4      8008.1\n",
      "        ...  \n",
      "715    5339.3\n",
      "716    5585.0\n",
      "717    5855.3\n",
      "718    5753.4\n",
      "719    6376.9\n",
      "Name: close, Length: 720, dtype: float64\n",
      "          0       1       2       3       4       5       6       7       8  \\\n",
      "0    8527.8  8364.9  8062.1  8163.3  8008.1  8422.1  8413.6  8282.5  8671.0   \n",
      "1    8364.9  8062.1  8163.3  8008.1  8422.1  8413.6  8282.5  8671.0  8953.2   \n",
      "2    8062.1  8163.3  8008.1  8422.1  8413.6  8282.5  8671.0  8953.2  8912.0   \n",
      "3    8163.3  8008.1  8422.1  8413.6  8282.5  8671.0  8953.2  8912.0  8927.0   \n",
      "4    8008.1  8422.1  8413.6  8282.5  8671.0  8953.2  8912.0  8927.0  8687.2   \n",
      "..      ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
      "701  5304.8  5212.0  5338.9  5274.1  5290.8  5208.2  5797.6  5837.4  5830.0   \n",
      "702  5212.0  5338.9  5274.1  5290.8  5208.2  5797.6  5837.4  5830.0  5765.1   \n",
      "703  5338.9  5274.1  5290.8  5208.2  5797.6  5837.4  5830.0  5765.1  5552.7   \n",
      "704  5274.1  5290.8  5208.2  5797.6  5837.4  5830.0  5765.1  5552.7  5586.8   \n",
      "705  5290.8  5208.2  5797.6  5837.4  5830.0  5765.1  5552.7  5586.8  5515.6   \n",
      "\n",
      "          9      10      11      12      13      14  \n",
      "0    8953.2  8912.0  8927.0  8687.2  8610.1  9056.9  \n",
      "1    8912.0  8927.0  8687.2  8610.1  9056.9  8621.3  \n",
      "2    8927.0  8687.2  8610.1  9056.9  8621.3  8143.4  \n",
      "3    8687.2  8610.1  9056.9  8621.3  8143.4  7937.6  \n",
      "4    8610.1  9056.9  8621.3  8143.4  7937.6  8064.7  \n",
      "..      ...     ...     ...     ...     ...     ...  \n",
      "701  5765.1  5552.7  5586.8  5515.6  5746.7  5339.3  \n",
      "702  5552.7  5586.8  5515.6  5746.7  5339.3  5585.0  \n",
      "703  5586.8  5515.6  5746.7  5339.3  5585.0  5855.3  \n",
      "704  5515.6  5746.7  5339.3  5585.0  5855.3  5753.4  \n",
      "705  5746.7  5339.3  5585.0  5855.3  5753.4  6376.9  \n",
      "\n",
      "[706 rows x 15 columns]\n",
      "                 0             1             2             3             4   \\\n",
      "count    706.000000    706.000000    706.000000    706.000000    706.000000   \n",
      "mean    6405.555807   6400.853824   6397.217422   6394.066289   6390.761331   \n",
      "std     2069.080207   2068.022191   2066.820538   2065.974357   2065.006157   \n",
      "min     2822.600000   2822.600000   2822.600000   2822.600000   2822.600000   \n",
      "25%     5051.775000   5051.775000   5051.775000   5051.775000   5051.775000   \n",
      "50%     6405.300000   6400.550000   6398.700000   6393.750000   6377.550000   \n",
      "75%     8007.825000   8006.675000   8004.325000   8000.125000   7990.850000   \n",
      "max    11221.300000  11221.300000  11221.300000  11221.300000  11221.300000   \n",
      "\n",
      "                 5             6             7             8             9   \\\n",
      "count    706.000000    706.000000    706.000000    706.000000    706.000000   \n",
      "mean    6387.584278   6383.519972   6379.516006   6375.596884   6371.454816   \n",
      "std     2064.239628   2063.052620   2061.849830   2060.856941   2059.174975   \n",
      "min     2822.600000   2822.600000   2822.600000   2822.600000   2822.600000   \n",
      "25%     5051.775000   5051.775000   5051.775000   5051.775000   5051.775000   \n",
      "50%     6358.150000   6346.450000   6335.800000   6328.000000   6324.000000   \n",
      "75%     7963.050000   7955.225000   7935.325000   7924.450000   7910.825000   \n",
      "max    11221.300000  11221.300000  11221.300000  11221.300000  11221.300000   \n",
      "\n",
      "                 10            11            12            13            14  \n",
      "count    706.000000    706.000000    706.000000    706.000000    706.000000  \n",
      "mean    6366.335977   6361.623513   6357.272663   6353.117139   6349.953966  \n",
      "std     2057.238922   2055.208898   2053.020536   2051.266241   2049.502009  \n",
      "min     2822.600000   2822.600000   2822.600000   2822.600000   2822.600000  \n",
      "25%     5051.775000   5051.775000   5051.775000   5051.775000   5051.775000  \n",
      "50%     6320.650000   6315.250000   6308.450000   6301.350000   6301.350000  \n",
      "75%     7905.375000   7902.300000   7900.950000   7895.950000   7888.000000  \n",
      "max    11221.300000  11221.300000  11221.300000  11221.300000  11221.300000  \n"
     ]
    }
   ],
   "source": [
    "# Prepare the non-normalized data\n",
    "print(df_num[CLOSE])\n",
    "df_series = prepare(list(df_num[CLOSE]), n)\n",
    "\n",
    "print(df_series)\n",
    "print(df_series.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale data\n",
    "scaler = preprocessing.StandardScaler()\n",
    "df_norm = pd.DataFrame(scaler.fit_transform(df_num.values), columns=df_num.columns, index=df_num.index)\n",
    "\n",
    "# Unscale data\n",
    "def unscale(value, key=CLOSE):\n",
    "    return value * np.std(df_num[key]) + np.mean(df_num[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      1.041922\n",
      "1      0.962462\n",
      "2      0.814761\n",
      "3      0.864125\n",
      "4      0.788421\n",
      "         ...   \n",
      "715   -0.513376\n",
      "716   -0.393528\n",
      "717   -0.261680\n",
      "718   -0.311385\n",
      "719   -0.007252\n",
      "Name: close, Length: 720, dtype: float64\n",
      "            0         1         2         3         4         5         6  \\\n",
      "0    1.041922  0.962462  0.814761  0.864125  0.788421  0.990363  0.986217   \n",
      "1    0.962462  0.814761  0.864125  0.788421  0.990363  0.986217  0.922268   \n",
      "2    0.814761  0.864125  0.788421  0.990363  0.986217  0.922268  1.111772   \n",
      "3    0.864125  0.788421  0.990363  0.986217  0.922268  1.111772  1.249425   \n",
      "4    0.788421  0.990363  0.986217  0.922268  1.111772  1.249425  1.229328   \n",
      "..        ...       ...       ...       ...       ...       ...       ...   \n",
      "701 -0.530205 -0.575471 -0.513571 -0.545180 -0.537034 -0.577325 -0.289825   \n",
      "702 -0.575471 -0.513571 -0.545180 -0.537034 -0.577325 -0.289825 -0.270411   \n",
      "703 -0.513571 -0.545180 -0.537034 -0.577325 -0.289825 -0.270411 -0.274021   \n",
      "704 -0.545180 -0.537034 -0.577325 -0.289825 -0.270411 -0.274021 -0.305678   \n",
      "705 -0.537034 -0.577325 -0.289825 -0.270411 -0.274021 -0.305678 -0.409283   \n",
      "\n",
      "            7         8         9        10        11        12        13  \\\n",
      "0    0.922268  1.111772  1.249425  1.229328  1.236645  1.119674  1.082066   \n",
      "1    1.111772  1.249425  1.229328  1.236645  1.119674  1.082066  1.300008   \n",
      "2    1.249425  1.229328  1.236645  1.119674  1.082066  1.300008  1.087530   \n",
      "3    1.229328  1.236645  1.119674  1.082066  1.300008  1.087530  0.854418   \n",
      "4    1.236645  1.119674  1.082066  1.300008  1.087530  0.854418  0.754032   \n",
      "..        ...       ...       ...       ...       ...       ...       ...   \n",
      "701 -0.270411 -0.274021 -0.305678 -0.409283 -0.392650 -0.427380 -0.314653   \n",
      "702 -0.274021 -0.305678 -0.409283 -0.392650 -0.427380 -0.314653 -0.513376   \n",
      "703 -0.305678 -0.409283 -0.392650 -0.427380 -0.314653 -0.513376 -0.393528   \n",
      "704 -0.409283 -0.392650 -0.427380 -0.314653 -0.513376 -0.393528 -0.261680   \n",
      "705 -0.392650 -0.427380 -0.314653 -0.513376 -0.393528 -0.261680 -0.311385   \n",
      "\n",
      "           14  \n",
      "0    1.300008  \n",
      "1    1.087530  \n",
      "2    0.854418  \n",
      "3    0.754032  \n",
      "4    0.816029  \n",
      "..        ...  \n",
      "701 -0.513376  \n",
      "702 -0.393528  \n",
      "703 -0.261680  \n",
      "704 -0.311385  \n",
      "705 -0.007252  \n",
      "\n",
      "[706 rows x 15 columns]\n",
      "               0           1           2           3           4           5   \\\n",
      "count  706.000000  706.000000  706.000000  706.000000  706.000000  706.000000   \n",
      "mean     0.006726    0.004432    0.002659    0.001121   -0.000491   -0.002040   \n",
      "std      1.009263    1.008747    1.008161    1.007748    1.007276    1.006902   \n",
      "min     -1.740981   -1.740981   -1.740981   -1.740981   -1.740981   -1.740981   \n",
      "25%     -0.653626   -0.653626   -0.653626   -0.653626   -0.653626   -0.653626   \n",
      "50%      0.006601    0.004284    0.003382    0.000967   -0.006935   -0.016398   \n",
      "75%      0.788286    0.787726    0.786579    0.784531    0.780006    0.766446   \n",
      "max      2.355767    2.355767    2.355767    2.355767    2.355767    2.355767   \n",
      "\n",
      "               6           7           8           9           10          11  \\\n",
      "count  706.000000  706.000000  706.000000  706.000000  706.000000  706.000000   \n",
      "mean    -0.004023   -0.005976   -0.007888   -0.009908   -0.012405   -0.014704   \n",
      "std      1.006323    1.005737    1.005252    1.004432    1.003487    1.002497   \n",
      "min     -1.740981   -1.740981   -1.740981   -1.740981   -1.740981   -1.740981   \n",
      "25%     -0.653626   -0.653626   -0.653626   -0.653626   -0.653626   -0.653626   \n",
      "50%     -0.022105   -0.027300   -0.031105   -0.033056   -0.034690   -0.037324   \n",
      "75%      0.762629    0.752922    0.747618    0.740971    0.738313    0.736813   \n",
      "max      2.355767    2.355767    2.355767    2.355767    2.355767    2.355767   \n",
      "\n",
      "               12          13          14  \n",
      "count  706.000000  706.000000  706.000000  \n",
      "mean    -0.016826   -0.018853   -0.020396  \n",
      "std      1.001430    1.000574    0.999713  \n",
      "min     -1.740981   -1.740981   -1.740981  \n",
      "25%     -0.653626   -0.653626   -0.653626  \n",
      "50%     -0.040641   -0.044104   -0.044104  \n",
      "75%      0.736155    0.733716    0.729838  \n",
      "max      2.355767    2.355767    2.355767  \n"
     ]
    }
   ],
   "source": [
    "# Prepare the normalized data\n",
    "print(df_norm[CLOSE])\n",
    "df_series_norm = prepare(list(df_norm[CLOSE]), n)\n",
    "\n",
    "print(df_series_norm)\n",
    "print(df_series_norm.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            0         1         2         3         4         5         6  \\\n",
      "0    1.041922  0.962462  0.814761  0.864125  0.788421  0.990363  0.986217   \n",
      "1    0.962462  0.814761  0.864125  0.788421  0.990363  0.986217  0.922268   \n",
      "2    0.814761  0.864125  0.788421  0.990363  0.986217  0.922268  1.111772   \n",
      "3    0.864125  0.788421  0.990363  0.986217  0.922268  1.111772  1.249425   \n",
      "4    0.788421  0.990363  0.986217  0.922268  1.111772  1.249425  1.229328   \n",
      "..        ...       ...       ...       ...       ...       ...       ...   \n",
      "701 -0.530205 -0.575471 -0.513571 -0.545180 -0.537034 -0.577325 -0.289825   \n",
      "702 -0.575471 -0.513571 -0.545180 -0.537034 -0.577325 -0.289825 -0.270411   \n",
      "703 -0.513571 -0.545180 -0.537034 -0.577325 -0.289825 -0.270411 -0.274021   \n",
      "704 -0.545180 -0.537034 -0.577325 -0.289825 -0.270411 -0.274021 -0.305678   \n",
      "705 -0.537034 -0.577325 -0.289825 -0.270411 -0.274021 -0.305678 -0.409283   \n",
      "\n",
      "            7         8         9        10        11        12        13  \n",
      "0    0.922268  1.111772  1.249425  1.229328  1.236645  1.119674  1.082066  \n",
      "1    1.111772  1.249425  1.229328  1.236645  1.119674  1.082066  1.300008  \n",
      "2    1.249425  1.229328  1.236645  1.119674  1.082066  1.300008  1.087530  \n",
      "3    1.229328  1.236645  1.119674  1.082066  1.300008  1.087530  0.854418  \n",
      "4    1.236645  1.119674  1.082066  1.300008  1.087530  0.854418  0.754032  \n",
      "..        ...       ...       ...       ...       ...       ...       ...  \n",
      "701 -0.270411 -0.274021 -0.305678 -0.409283 -0.392650 -0.427380 -0.314653  \n",
      "702 -0.274021 -0.305678 -0.409283 -0.392650 -0.427380 -0.314653 -0.513376  \n",
      "703 -0.305678 -0.409283 -0.392650 -0.427380 -0.314653 -0.513376 -0.393528  \n",
      "704 -0.409283 -0.392650 -0.427380 -0.314653 -0.513376 -0.393528 -0.261680  \n",
      "705 -0.392650 -0.427380 -0.314653 -0.513376 -0.393528 -0.261680 -0.311385  \n",
      "\n",
      "[706 rows x 14 columns]\n",
      "0      1.300008\n",
      "1      1.087530\n",
      "2      0.854418\n",
      "3      0.754032\n",
      "4      0.816029\n",
      "         ...   \n",
      "701   -0.513376\n",
      "702   -0.393528\n",
      "703   -0.261680\n",
      "704   -0.311385\n",
      "705   -0.007252\n",
      "Name: 14, Length: 706, dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPRegressor(alpha=1, solver='lbfgs')"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train model\n",
    "\n",
    "X = df_series_norm.iloc[:,:n-1]\n",
    "y = df_series_norm.iloc[:,n-1]\n",
    "\n",
    "print(X)\n",
    "print(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y.values.ravel())\n",
    "model = neural_network.MLPRegressor(hidden_layer_sizes=(100,),\n",
    "                                    activation='relu',\n",
    "                                    solver='lbfgs',\n",
    "                                    alpha=1\n",
    "                                   )\n",
    "\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "def absolute_error_score(y_true, y_pred):\n",
    "    diff = np.abs(unscale(y_true) - unscale(y_pred))\n",
    "    return -np.average(diff, axis=0)\n",
    "\n",
    "def absolute_error(estimator, X, y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "    estimator.fit(X_train, y_train)\n",
    "    y_pred = estimator.predict(X_test)\n",
    "    return absolute_error_score(y_test, y_pred)\n",
    "\n",
    "def percentage_error_score(y_true, y_pred):\n",
    "    percentage = (np.abs(y_pred - y_true)) / np.abs(y_true) * 100\n",
    "    return -np.average(percentage, axis=0)\n",
    "\n",
    "def percentage_error(estimator, X, y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "    estimator.fit(X_train, y_train)\n",
    "    y_pred = estimator.predict(X_test)\n",
    "    return percentage_error_score(y_test, y_pred)\n",
    "\n",
    "def aon(x_close, y_pred, y_true):\n",
    "    # all-or-none principle\n",
    "    return not ((y_pred <= x_close) != (y_true < x_close))\n",
    "        \n",
    "def aon_scorer(estimator, X, y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "    estimator.fit(X_train, y_train)\n",
    "    y_pred = estimator.predict(X_test).reshape(X_test.shape[0], 1)\n",
    "    X_estimator = X_test.copy()\n",
    "    X_estimator.insert(n-1, 'y_test', y_test)\n",
    "    X_estimator.insert(n, 'y_pred', y_pred)\n",
    "    aon_series = X_estimator.apply(lambda val: aon(val[n-2], val['y_pred'], val['y_test']), axis=1)\n",
    "    return np.mean(aon_series)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-199.19342228995237 41.526153734084694\n",
      "-43.61534397708244 38.65184388183109\n",
      "0.47777777777777775 0.07934920476158723\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'fit_time': array([1.00143647, 0.70846081, 0.66917539, 0.59097886, 0.53923655]),\n",
       " 'score_time': array([1.05541229, 0.60668349, 0.99685073, 0.63606215, 0.36306119]),\n",
       " 'test_percentage_error': array([-91.32848138, -89.54203842, -13.25914335,  -3.0261118 ,\n",
       "        -20.92094493]),\n",
       " 'test_absolute_error': array([-200.74962567, -172.00834817, -269.33009999, -145.54659607,\n",
       "        -208.33244155]),\n",
       " 'test_aon_scorer': array([0.61111111, 0.52777778, 0.41666667, 0.41666667, 0.41666667])}"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = neural_network.MLPRegressor(hidden_layer_sizes=(100,),\n",
    "                                    activation='relu',\n",
    "                                    solver='lbfgs',\n",
    "                                    alpha=1\n",
    "                                   )\n",
    "\n",
    "scores = cross_validate(model, X, y,\n",
    "                        scoring={\n",
    "                            'percentage_error': percentage_error,\n",
    "                            'absolute_error': absolute_error,\n",
    "                            'aon_scorer': aon_scorer\n",
    "                        })\n",
    "\n",
    "print(np.mean(scores['test_absolute_error']), np.std(scores['test_absolute_error']))\n",
    "print(np.mean(scores['test_percentage_error']), np.std(scores['test_percentage_error']))\n",
    "print(np.mean(scores['test_aon_scorer']), np.std(scores['test_aon_scorer']))\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-198.09290110353498 61.73881179831931\n",
      "-70.3341976879078 100.56899020868951\n",
      "0.5 0.07027283689263066\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'fit_time': array([0.10888863, 0.06552911, 0.05637002, 0.04784989, 0.09430027]),\n",
       " 'score_time': array([0.24875998, 0.26210237, 0.28466821, 0.32289028, 0.22849584]),\n",
       " 'test_percentage_error': array([ -44.37533827,  -15.97804273,  -15.20099991,   -6.28316118,\n",
       "        -269.83344635]),\n",
       " 'test_absolute_error': array([-287.56229057, -187.58375022, -245.08827672, -155.14278487,\n",
       "        -115.08740313]),\n",
       " 'test_aon_scorer': array([0.52777778, 0.55555556, 0.52777778, 0.36111111, 0.52777778])}"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = neural_network.MLPRegressor(hidden_layer_sizes=(10,10,10),\n",
    "                                    activation='identity',\n",
    "                                    solver='lbfgs',\n",
    "                                    alpha=0.01\n",
    "                                   )\n",
    "\n",
    "scores = cross_validate(model, X, y,\n",
    "                        scoring={\n",
    "                            'percentage_error': percentage_error,\n",
    "                            'absolute_error': absolute_error,\n",
    "                            'aon_scorer': aon_scorer\n",
    "                        })\n",
    "\n",
    "print(np.mean(scores['test_absolute_error']), np.std(scores['test_absolute_error']))\n",
    "print(np.mean(scores['test_percentage_error']), np.std(scores['test_percentage_error']))\n",
    "print(np.mean(scores['test_aon_scorer']), np.std(scores['test_aon_scorer']))\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'identity', 'alpha': 0.01, 'hidden_layer_sizes': (100,), 'solver': 'lbfgs'}\n",
      "-191.23155356663693\n"
     ]
    }
   ],
   "source": [
    "############################################# Search the best parameters\n",
    "model = neural_network.MLPRegressor()\n",
    "\n",
    "gs = GridSearchCV(\n",
    "    model,\n",
    "    {\n",
    "        'activation': ['identity', 'logistic', 'tanh', 'relu'],\n",
    "        'alpha': 10.0**np.arange(-2,2),\n",
    "        'solver': ['lbfgs', 'sgd', 'adam'],\n",
    "        'hidden_layer_sizes': [(100,), (10, 10, 10), (125, 125)]\n",
    "    },\n",
    "    scoring=absolute_error\n",
    ")\n",
    "\n",
    "gs.fit(X_train, y_train)\n",
    "print(gs.best_params_)\n",
    "print(gs.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'tanh', 'alpha': 0.1, 'hidden_layer_sizes': (100,), 'solver': 'lbfgs'}\n",
      "-30.876364804634445\n"
     ]
    }
   ],
   "source": [
    "############################################# Search the best parameters\n",
    "model = neural_network.MLPRegressor()\n",
    "\n",
    "gs = GridSearchCV(\n",
    "    model,\n",
    "    {\n",
    "        'activation': ['identity', 'logistic', 'tanh', 'relu'],\n",
    "        'alpha': 10.0**np.arange(-2,2),\n",
    "        'solver': ['lbfgs', 'sgd', 'adam'],\n",
    "        'hidden_layer_sizes': [(100,), (10, 10, 10), (125, 125)]\n",
    "    },\n",
    "    scoring=percentage_error\n",
    ")\n",
    "\n",
    "gs.fit(X_train, y_train)\n",
    "print(gs.best_params_)\n",
    "print(gs.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'identity', 'alpha': 0.01, 'hidden_layer_sizes': (10, 10, 10), 'solver': 'lbfgs'}\n",
      "0.4888888888888889\n"
     ]
    }
   ],
   "source": [
    "############################################# Search the best parameters\n",
    "model = neural_network.MLPRegressor()\n",
    "\n",
    "gs = GridSearchCV(\n",
    "    model,\n",
    "    {\n",
    "        'activation': ['identity', 'logistic', 'tanh', 'relu'],\n",
    "        'alpha': 10.0**np.arange(-2,2),\n",
    "        'solver': ['lbfgs', 'sgd', 'adam'],\n",
    "        'hidden_layer_sizes': [(100,), (10, 10, 10), (125, 125)]\n",
    "    },\n",
    "    scoring=aon_scorer\n",
    ")\n",
    "\n",
    "gs.fit(X_train, y_train)\n",
    "print(gs.best_params_)\n",
    "print(gs.score(X_test, y_test))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMhA+pJA1OwJUcM4iHbaLlF",
   "collapsed_sections": [],
   "name": "Untitled0.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
